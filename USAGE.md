# Web Crawler Usage Guide

## Overview

This web crawler application is built with Scrapy and provides a comprehensive solution for crawling websites, downloading files, and managing content with proper politeness and respect for robots.txt. It now includes **Playwright integration** for handling Single Page Applications (SPAs) and JavaScript-heavy websites.

## Features

- **Configurable crawling**: YAML-based configuration
- **SPA Support**: Playwright integration for JavaScript-heavy sites
- **Politeness**: Respects robots.txt, rate limiting, and domain limits
- **File downloads**: Downloads images, PDFs, and other specified file types
- **Content deduplication**: Hash-based content deduplication
- **Depth control**: Configurable crawling depth
- **Domain management**: Per-domain page limits and allowed domains
- **Exception handling**: Exclude unwanted URLs with regex patterns
- **Manifest generation**: JSON manifest of all downloaded files
- **HTML Page Downloads**: Downloads HTML pages with proper directory structure
- **Domain-based Organization**: Creates domain-specific folders (e.g., thesceptreai.com)
- **Dynamic Slowdown**: Adaptive rate limiting to bypass anti-bot measures
- **User Agent Rotation**: Random user agent selection for better stealth

## Quick Start

### 1. Setup

```bash
# Activate virtual environment
source .venv/bin/activate

# Install dependencies (if not already done)
pip install -r requirements.txt

# Install Playwright browsers (for SPA support)
playwright install chromium
```

### 2. Configuration

Edit `config.yml` to set your target website and preferences:

```yaml
crawler:
  base_url: "https://thesceptreai.com"
  allowed_domains:
    - "thesceptreai.com"
    - "www.thesceptreai.com"
  exclude_patterns:
    - ".*/about.*"
    - ".*/ads.*"
    - ".*\\.(css|js|ico)$"
  max_depth: 5
  max_pages_per_domain: 200
  # SPA-optimized settings
  delay_between_requests: 1.0
  concurrent_requests: 4
  concurrent_requests_per_domain: 2
```

### 3. Run the Crawler

```bash
# Basic usage
python app.py

# With custom URL
python app.py --url "https://thesceptreai.com"

# With verbose logging
python app.py --verbose

# For Single Page Applications (SPAs) - RECOMMENDED
python app.py --url "https://thesceptreai.com" --playwright --verbose

# With custom config file
python app.py --config my_config.yml --playwright
```

## SPA (Single Page Application) Support

### When to Use Playwright

Use the `--playwright` flag for:
- React, Vue, Angular, or other SPA frameworks
- Sites that load content dynamically with JavaScript
- Sites that require user interaction to reveal content
- Sites with client-side routing
- Sites that show empty content without JavaScript execution

### SPA Crawling Features

- **JavaScript Execution**: Waits for JavaScript to load and execute
- **Network Idle Detection**: Waits for all network requests to complete
- **Dynamic Content Loading**: Handles content loaded after initial page load
- **Client-Side Routing**: Follows links generated by JavaScript
- **Extended Wait Times**: Configurable delays for slow-loading content

### Example: SPA Crawling

```bash
# Crawl a React-based website
python app.py --url "https://thesceptreai.com" --playwright --verbose

# Expected output:
# - Downloads fully rendered HTML content
# - Follows JavaScript-generated links
# - Captures dynamic content
# - Creates proper directory structure
```

## Configuration Options

### Basic Settings

- `base_url`: Starting URL for crawling
- `allowed_domains`: List of domains to crawl
- `exclude_patterns`: Regex patterns for URLs to skip
- `max_depth`: Maximum crawling depth (default: 5 for SPAs)
- `max_pages_per_domain`: Maximum pages per domain (default: 200 for SPAs)

### SPA-Specific Settings

- `delay_between_requests`: Increased delay for SPA sites (1.0s recommended)
- `concurrent_requests`: Reduced concurrency for SPA sites (4 recommended)
- `concurrent_requests_per_domain`: Reduced per-domain concurrency (2 recommended)

### File Download Settings

- `download_file_types`: MIME types to download
- `max_file_size_mb`: Maximum file size in MB

### Politeness Settings

- `delay_between_requests`: Delay between requests in seconds
- `concurrent_requests`: Number of concurrent requests
- `concurrent_requests_per_domain`: Concurrent requests per domain

### Dynamic Slowdown Settings

```yaml
dynamic_slowdown:
  enabled: true
  min_delay: 1.0      # Minimum delay in seconds
  max_delay: 3.0      # Maximum delay in seconds
  progressive: true   # Increase delay for subsequent requests
  per_domain: true    # Apply different delays per domain
```

### Storage Settings

- `output_dir`: Directory for downloaded files
- `manifest_file`: JSON file with download manifest
- `log_file`: Log file path

## Command Line Options

```bash
python app.py [OPTIONS]

Options:
  -h, --help            Show help message
  -c, --config CONFIG   Configuration file path (default: config.yml)
  -u, --url URL         Custom URL to crawl (overrides config)
  -v, --verbose         Enable verbose logging
  -p, --playwright      Use Playwright for JavaScript-heavy sites (SPAs)
```

## Examples

### Example 1: Crawl a React SPA

```bash
# Create config for React SPA
cat > spa_config.yml << 'YAML_EOF'
crawler:
  base_url: "https://thesceptreai.com"
  allowed_domains:
    - "thesceptreai.com"
    - "www.thesceptreai.com"
  exclude_patterns:
    - ".*/admin/.*"
    - ".*\\.(css|js|ico|png|jpg|gif|svg|woff|woff2|ttf|eot)$"
    - ".*#.*" # Exclude fragments
    - ".*\\?.*utm_.*" # Exclude tracking parameters
  download_file_types:
    - "application/pdf"
    - "image/jpeg"
    - "image/png"
  max_depth: 5
  max_pages_per_domain: 200
  delay_between_requests: 1.0
  concurrent_requests: 4
  concurrent_requests_per_domain: 2
  dynamic_slowdown:
    enabled: true
    min_delay: 1.0
    max_delay: 3.0
    progressive: true
    per_domain: true
YAML_EOF

# Run crawler with Playwright
python app.py --config spa_config.yml --playwright --verbose
```

### Example 2: Crawl a Documentation Site

```bash
# Create config for documentation site
cat > doc_config.yml << 'YAML_EOF'
crawler:
  base_url: "https://docs.thesceptreai.com"
  allowed_domains:
    - "docs.thesceptreai.com"
  exclude_patterns:
    - ".*/search.*"
    - ".*/api/.*"
    - ".*\\.(css|js|ico|png|jpg)$"
  download_file_types:
    - "application/pdf"
    - "text/plain"
  max_depth: 2
  max_pages_per_domain: 50
YAML_EOF

# Run crawler (regular mode for static sites)
python app.py --config doc_config.yml --verbose
```

### Example 3: Download Images from a Gallery

```bash
# Create config for image gallery
cat > gallery_config.yml << 'YAML_EOF'
crawler:
  base_url: "https://gallery.thesceptreai.com"
  allowed_domains:
    - "gallery.thesceptreai.com"
  exclude_patterns:
    - ".*/thumbnails/.*"
    - ".*/admin/.*"
  download_file_types:
    - "image/jpeg"
    - "image/png"
    - "image/gif"
  max_depth: 1
  max_pages_per_domain: 200
YAML_EOF

# Run crawler
python app.py --config gallery_config.yml
```

### Example 4: Quick Test with Custom URL

```bash
# Test with a specific URL (SPA)
python app.py --url "https://thesceptreai.com" --playwright --verbose

# Test with a specific URL (regular site)
python app.py --url "https://httpbin.org" --verbose
```

## Output Structure

After running the crawler, you'll find:

```
downloads/
├── thesceptreai.com/
│   ├── index.html
│   ├── license.html
│   ├── tos.html
│   ├── cybersecurity.html
│   └── crawl_manifest.json
└── www.thesceptreai.com/
    └── index.html
```

### Directory Naming Convention

- Domain names are preserved as-is for better readability
- Example: `thesceptreai.com` becomes `thesceptreai.com`
- Example: `www.thesceptreai.com` becomes `www.thesceptreai.com`

### File Organization

- HTML pages are saved with proper directory hierarchy
- URLs like `https://thesceptreai.com/products/item1` become `thesceptreai.com/products/item1.html`
- Root pages become `index.html`
- Each domain gets its own folder with a `crawl_manifest.json` file

## Manifest File

The `crawl_manifest.json` contains metadata about all downloaded files:

```json
{
  "https://thesceptreai.com/": {
    "file_path": "thesceptreai.com/index.html",
    "hash": "a9b462cd74331a543858758d2a35169fbe75b76928712493e3fa958d74d8d418",
    "content_type": "text/html",
    "title": "SceptreAI - AI Powered Knowledge Base",
    "depth": 0,
    "timestamp": "2025-09-09T10:37:35.110826",
    "size": 22374
  },
  "https://thesceptreai.com/cybersecurity": {
    "file_path": "thesceptreai.com/cybersecurity.html",
    "hash": "c7de3ea55eb2636f00b61d72b546f2d17e36665512077ff91777c5d2a86fa0f2",
    "content_type": "text/html",
    "title": "SceptreAI - AI Powered Knowledge Base",
    "depth": 1,
    "timestamp": "2025-09-09T10:37:44.576543",
    "size": 25796
  }
}
```

## Performance Comparison

### Regular Crawling vs Playwright Crawling

| Feature | Regular Crawling | Playwright Crawling |
|---------|------------------|-------------------|
| **JavaScript Execution** | ❌ No | ✅ Yes |
| **Dynamic Content** | ❌ No | ✅ Yes |
| **SPA Support** | ❌ No | ✅ Yes |
| **Speed** | ⚡ Fast | 🐌 Slower |
| **Resource Usage** | 💚 Low | 🔴 Higher |
| **Content Quality** | �� Basic HTML | 📄 Full Rendered Content |

### Example Results

**Regular Crawling (without --playwright):**
```
Found 0 links on https://thesceptreai.com
Downloaded: 1 page (empty React shell)
Content: <div id="root"></div>
```

**Playwright Crawling (with --playwright):**
```
Found 4 links on https://thesceptreai.com
Downloaded: 4 pages with full content
Content: Complete rendered HTML with all dynamic content
```

## Best Practices

### 1. SPA Crawling

- Always use `--playwright` for JavaScript-heavy sites
- Increase delays for better reliability
- Reduce concurrency to avoid overwhelming the site
- Use appropriate wait times for slow-loading content

### 2. Respectful Crawling

- Always check robots.txt compliance
- Use appropriate delays between requests
- Set reasonable domain limits
- Identify your crawler with proper User-Agent
- Use dynamic slowdown for anti-bot protection

### 3. Configuration

- Start with small limits and increase gradually
- Use specific exclude patterns to avoid unwanted content
- Set appropriate file size limits
- Configure allowed domains carefully
- Test with `--verbose` to monitor behavior

### 4. Monitoring

- Use verbose logging for debugging
- Monitor the manifest file for download status
- Check logs for any errors or warnings
- Use depth limits to control crawl scope
- Watch for rate limiting or blocking

## Troubleshooting

### Common Issues

1. **Permission Denied**: Check file permissions for output directory
2. **Memory Issues**: Reduce concurrent requests or page limits
3. **Rate Limiting**: Increase delays between requests or enable dynamic slowdown
4. **Domain Blocked**: Check robots.txt and site policies
5. **Empty Content**: Use `--playwright` for JavaScript-heavy sites
6. **Playwright Not Found**: Run `playwright install chromium`

### Debug Mode

```bash
# Enable debug logging
python app.py --verbose --playwright

# Check logs
tail -f crawler.log

# Test with a simple site first
python app.py --url "https://httpbin.org" --verbose
```

### SPA-Specific Issues

1. **No Links Found**: Site may require JavaScript - use `--playwright`
2. **Empty Content**: Increase wait times in configuration
3. **Infinite Loops**: Check exclude patterns and domain limits
4. **Slow Performance**: Reduce concurrency and increase delays

## Advanced Usage

### Custom Pipelines

You can extend the crawler by adding custom pipelines in `pipelines.py`:

```python
class CustomPipeline:
    def process_item(self, item, spider):
        # Custom processing logic
        return item
```

### Custom Middleware

Add custom middleware in `middlewares.py`:

```python
class CustomMiddleware:
    def process_request(self, request, spider):
        # Custom request processing
        return None
```

### Playwright Configuration

Customize Playwright settings in `app.py`:

```python
# Custom Playwright settings
settings.set('PLAYWRIGHT_LAUNCH_OPTIONS', {
    'headless': True,
    'args': ['--no-sandbox', '--disable-dev-shm-usage']
})
```

## Legal and Ethical Considerations

- Always respect robots.txt
- Follow website terms of service
- Use appropriate delays and limits
- Don't overload servers
- Respect copyright and intellectual property
- Consider reaching out to site owners for large crawls
- Be mindful of data privacy regulations (GDPR, CCPA, etc.)

## Support

For issues or questions:
1. Check the logs in `crawler.log`
2. Review the configuration in `config.yml`
3. Test with a small scope first
4. Ensure all dependencies are installed
5. Verify Playwright browsers are installed: `playwright install chromium`
6. Try both regular and Playwright modes to identify the issue

## Changelog

### Version 2.0 (Current)
- ✅ Added Playwright integration for SPA support
- ✅ Enhanced JavaScript execution and waiting
- ✅ Improved recursive crawling for dynamic content
- ✅ Added dynamic slowdown middleware
- ✅ Added user agent rotation
- ✅ Enhanced configuration for SPA sites
- ✅ Better error handling and logging
- ✅ Improved directory structure and file organization
