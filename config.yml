# Web Crawler Configuration
crawler:
    # Base URL to start crawling from
    base_url: "https://example.com"

    # Allowed domains (can be multiple)
    allowed_domains:
        - "example.com"
        - "www.example.com"
        - "httpbin.org"
        - "www.httpbin.org"

    # URL patterns to exclude (regex patterns)
    exclude_patterns:
        - ".*/about.*"
        - ".*/advertisements.*"
        - ".*/ads.*"
        - ".*/advert.*"
        - ".*/privacy.*"
        - ".*/terms.*"
        - ".*/contact.*"
        - ".*/sitemap.*"
        - ".*/robots.txt"
        - ".*\\.(css|js|ico|png|jpg|jpeg|gif|svg|woff|woff2|ttf|eot)$"
        - ".*#.*" # Exclude fragments
        - ".*\\?.*utm_.*" # Exclude tracking parameters
        - ".*\\?.*fbclid.*" # Exclude Facebook tracking
        - ".*\\?.*gclid.*" # Exclude Google tracking

    # File types to download
    download_file_types:
        - "application/pdf"
        - "image/jpeg"
        - "image/png"
        - "image/gif"
        - "image/svg+xml"
        - "text/plain"
        - "application/msword"
        - "application/vnd.openxmlformats-officedocument.wordprocessingml.document"

    # Page download types - specify which page types to download
    page_download_types:
        - "html" # Download HTML pages
        # - "pdf" # Download PDF files
        # - "json"
        # - "doc"   # Download DOC files
        # - "docx"  # Download DOCX files
        # - "txt"   # Download text files
        # Add more types as needed: "xml", "json", "csv", etc.

    # Crawling limits
    max_depth: 3 # Increased for better SPA crawling
    max_pages_per_domain: 200 # Increased for comprehensive crawling
    max_file_size_mb: 50
    max_retries: 5 # Increased retries for better reliability

    # Politeness settings
    delay_between_requests: 2.0 # Increased delay for better reliability
    concurrent_requests: 2 # Reduced for better stability
    concurrent_requests_per_domain: 1 # Reduced for better stability

    # Timeout settings - Increased for better reliability
    timeout_settings:
        page_load_timeout: 60 # Increased from 30 to 60 seconds
        network_idle_timeout: 20 # Increased from 10 to 20 seconds
        javascript_timeout: 30 # Increased from 15 to 30 seconds
        request_timeout: 120 # Increased from 60 to 120 seconds
        retry_timeout: 10 # Increased from 5 to 10 seconds

    # Dynamic slowdown settings (to bypass rate limiters)
    dynamic_slowdown:
        enabled: true
        min_delay: 2.0 # Increased minimum delay
        max_delay: 5.0 # Increased maximum delay
        progressive: true # Increase delay for subsequent requests to same domain
        per_domain: true # Apply different delays per domain

    # User agent
    user_agent: "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

    # Storage settings
    storage:
        output_dir: "./downloads"
        manifest_file: "./crawl_manifest.json"
        log_file: "./crawler.log"
