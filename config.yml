# Web Crawler Configuration
crawler:
    # Base URL to start crawling from
    base_url: "https://example.com"

    # Allowed domains (can be multiple)
    allowed_domains:
        - "example.com"
        - "www.example.com"
        - "httpbin.org"
        - "www.httpbin.org"

    # URL patterns to exclude (regex patterns)
    exclude_patterns:
        - ".*/about.*"
        - ".*/advertisements.*"
        - ".*/ads.*"
        - ".*/advert.*"
        - ".*/privacy.*"
        - ".*/terms.*"
        - ".*/contact.*"
        - ".*/sitemap.*"
        - ".*/robots.txt"
        - ".*\\.(css|js|ico|png|jpg|jpeg|gif|svg|woff|woff2|ttf|eot)$"
        - ".*#.*" # Exclude fragments
        - ".*\\?.*utm_.*" # Exclude tracking parameters
        - ".*\\?.*fbclid.*" # Exclude Facebook tracking
        - ".*\\?.*gclid.*" # Exclude Google tracking

    # File types to download
    download_file_types:
        - "application/pdf"
        - "image/jpeg"
        - "image/png"
        - "image/gif"
        - "image/svg+xml"
        - "text/plain"
        - "application/msword"
        - "application/vnd.openxmlformats-officedocument.wordprocessingml.document"

    # Page download types - specify which page types to download
    page_download_types:
        - "html"  # Download HTML pages
        - "pdf"   # Download PDF files
        - "doc"   # Download DOC files
        - "docx"  # Download DOCX files
        - "txt"   # Download text files
        # Add more types as needed: "xml", "json", "csv", etc.

    # Crawling limits
    max_depth: 5 # Increased for better SPA crawling
    max_pages_per_domain: 200 # Increased for comprehensive crawling
    max_file_size_mb: 50
    max_retries: 3 # Maximum retries for failed requests

    # Politeness settings
    delay_between_requests: 1.0 # Increased delay for SPA sites
    concurrent_requests: 4 # Reduced for SPA sites
    concurrent_requests_per_domain: 2 # Reduced for SPA sites

    # Timeout settings
    timeout_settings:
        page_load_timeout: 30 # Maximum time to wait for page load (seconds)
        network_idle_timeout: 10 # Maximum time to wait for network idle (seconds)
        javascript_timeout: 15 # Maximum time to wait for JavaScript execution (seconds)
        request_timeout: 60 # Maximum time for individual requests (seconds)
        retry_timeout: 5 # Time to wait before retrying failed requests (seconds)

    # Dynamic slowdown settings (to bypass rate limiters)
    dynamic_slowdown:
        enabled: true
        min_delay: 1.0 # Minimum delay in seconds
        max_delay: 3.0 # Maximum delay in seconds
        progressive: true # Increase delay for subsequent requests to same domain
        per_domain: true # Apply different delays per domain

    # User agent
    user_agent: "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

    # Storage settings
    storage:
        output_dir: "./downloads"
        manifest_file: "./crawl_manifest.json"
        log_file: "./crawler.log"
